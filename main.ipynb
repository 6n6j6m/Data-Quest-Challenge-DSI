{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e55440d",
   "metadata": {},
   "source": [
    "# Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24dc3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import optuna\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"Seed set to {seed}\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([1, 2, 2, 3])\n",
    "\n",
    "# def one_hot_encode(X):\n",
    "#     unique_categories = np.unique(X)\n",
    "#     # np.unique already returns sorted unique values for most comparable types\n",
    "\n",
    "#     n_objects = len(X)\n",
    "#     n_unique_categories = len(unique_categories)\n",
    "\n",
    "#     # 3. Create an Empty One-Hot Matrix\n",
    "#     # Initialize with zeros. Use dtype=int for 0s and 1s.\n",
    "#     one_hot_matrix = np.zeros((n_objects, n_unique_categories), dtype=int)\n",
    "\n",
    "#     # Create a mapping from category value to its column index\n",
    "#     # This makes lookup efficient\n",
    "#     category_to_index = {category: i for i, category in enumerate(unique_categories)}\n",
    "\n",
    "#     # 4. Populate the Matrix\n",
    "#     for i, value in enumerate(X):\n",
    "#         col_index = category_to_index[value]\n",
    "#         one_hot_matrix[i, col_index] = 1\n",
    "\n",
    "#     return one_hot_matrix\n",
    "\n",
    "# print(one_hot_encode(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/Data Quest Challenge DSI/training_dataset.csv')\n",
    "test = pd.read_csv('D:/Data Quest Challenge DSI/validation_set.csv')\n",
    "\n",
    "train.drop(['customer_number'], inplace=True, axis=1)\n",
    "test.drop(['customer_number'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.info())\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in both train and test datasets\n",
    "print(\"Missing values in training dataset:\")\n",
    "print(train.isna().sum())\n",
    "\n",
    "print(\"\\nMissing values in validation dataset:\")\n",
    "print(test.isna().sum())\n",
    "\n",
    "# Print total number of rows with missing values in each dataset\n",
    "print(\"\\nTotal rows with missing values:\")\n",
    "print(f\"Training dataset: {train.isna().any(axis=1).sum()} out of {train.shape[0]} rows\")\n",
    "print(f\"Validation dataset: {test.isna().any(axis=1).sum()} out of {test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb148c",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f889b7",
   "metadata": {},
   "source": [
    "gagal_bayar_sebelumnya cuman ada 2 baris yang classnya yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd0906",
   "metadata": {},
   "source": [
    "there's no class for yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abd1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = train.select_dtypes(include=['object']).columns.tolist()\n",
    "num_col = train.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", cat_col)\n",
    "print(f\"Jumlah kolom kategorikal: { len(cat_col) }\\n\")\n",
    "\n",
    "print(\"Numerical columns:\", num_col)\n",
    "print(\"Jumlah kolom numerikal:\", len(num_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique Value pendidikan:\", train['pendidikan'].unique())\n",
    "print(\"Unique Value pekerjaan:\", train['pekerjaan'].unique())\n",
    "print(\"Unique Value status_perkawinan:\", train['status_perkawinan'].unique())\n",
    "print(\"Unique Value bulan_kontak_terakhir:\", train['bulan_kontak_terakhir'].unique())\n",
    "print(\"Unique Value hari_kontak_terakhir:\", train['hari_kontak_terakhir'].unique())\n",
    "print(\"Unique Value pulau:\", train['pulau'].unique())\n",
    "\n",
    "# Lookin the distribution of each categorical variable\n",
    "def plot_categorical_distribution(df, column):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.countplot(data=df, x=column, order=df[column].value_counts().index)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_distribution(train, 'pendidikan')\n",
    "plot_categorical_distribution(train, 'pekerjaan')\n",
    "plot_categorical_distribution(train, 'status_perkawinan')\n",
    "plot_categorical_distribution(train, 'bulan_kontak_terakhir')\n",
    "plot_categorical_distribution(train, 'hari_kontak_terakhir')\n",
    "plot_categorical_distribution(train, 'pulau')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54409901",
   "metadata": {},
   "source": [
    "Merubah semua kategori unknown menjadi NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace unknown values with NaN\n",
    "train['pendidikan'] = train['pendidikan'].replace(['unknown'], np.nan)\n",
    "train['pekerjaan'] = train['pekerjaan'].replace(['unknown'], np.nan)\n",
    "train['status_perkawinan'] = train['status_perkawinan'].replace(['unknown'], np.nan)\n",
    "train['gagal_bayar_sebelumnya'] = train['gagal_bayar_sebelumnya'].replace(['unknown'], np.nan)\n",
    "train['pinjaman_rumah'] = train['pinjaman_rumah'].replace(['unknown'], np.nan)\n",
    "train['pinjaman_pribadi'] = train['pinjaman_pribadi'].replace(['unknown'], np.nan)\n",
    "train['pinjaman_pribadi'] = train['pinjaman_pribadi'].replace(['unknown'], np.nan)\n",
    "\n",
    "test['pendidikan'] = test['pendidikan'].replace(['unknown'], np.nan)\n",
    "test['pekerjaan'] = test['pekerjaan'].replace(['unknown'], np.nan)\n",
    "test['status_perkawinan'] = test['status_perkawinan'].replace(['unknown'], np.nan)\n",
    "test['gagal_bayar_sebelumnya'] = test['gagal_bayar_sebelumnya'].replace(['unknown'], np.nan)\n",
    "test['pinjaman_rumah'] = test['pinjaman_rumah'].replace(['unknown'], np.nan)\n",
    "test['pinjaman_pribadi'] = test['pinjaman_pribadi'].replace(['unknown'], np.nan)\n",
    "\n",
    "# Replace 999 with 0\n",
    "train['hari_sejak_kontak_sebelumnya'] = train['hari_sejak_kontak_sebelumnya'].replace([999], 0)\n",
    "test['hari_sejak_kontak_sebelumnya'] = test['hari_sejak_kontak_sebelumnya'].replace([999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd983dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in training dataset:\")\n",
    "print(train.isna().sum())\n",
    "print(\"Duplicate rows in training dataset:\")\n",
    "print(train.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values in validation dataset:\")\n",
    "print(test.isna().sum())\n",
    "print(\"Duplicate rows in validation dataset:\")\n",
    "print(test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ed638",
   "metadata": {},
   "source": [
    "Outliers checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for numerical columns \n",
    "num_cols = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "print(\"Numerical columns in training dataset:\", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a larger size for all numerical columns\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Create a boxplot for each numerical column\n",
    "sns.boxplot(data=train['usia'], orient='h')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Boxplots of Numerical Variables')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b25e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Threshold atas\n",
    "# def replace_values_above_threshold(data_train, column, threshold):\n",
    "#     sns.boxplot(data_train[column])\n",
    "#     plt.title(f'Original Box Plot of {column}')\n",
    "#     plt.show()\n",
    "\n",
    "#     above_threshold = data_train[column] > threshold\n",
    "#     data_train.loc[above_threshold, column] = threshold\n",
    "\n",
    "#     sns.boxplot(data_train[column])\n",
    "#     plt.title(f'Box Plot with Values Replaced above {threshold}')\n",
    "#     plt.show()\n",
    "\n",
    "#     return train\n",
    "\n",
    "# train = replace_values_above_threshold(train, 'usia', 68)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87715aec",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0013ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop_duplicates(inplace=True)\n",
    "test.drop_duplicates(inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "print(\"Duplicate rows in training dataset after removal:\")\n",
    "print(train.duplicated().sum())\n",
    "print(\"Duplicate rows in validation dataset after removal:\")\n",
    "print(test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154c52e",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f36401",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "def one_hot_encode(df, columns):\n",
    "    return pd.get_dummies(df, columns=columns)\n",
    "\n",
    "# Label pekerjaan\n",
    "def label_encode(df, column):\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Mapping Ordinal Columns\n",
    "pendidikan_mapping = {\n",
    "    'TIDAK SEKOLAH': 0,\n",
    "    'Tidak Tamat SD': 1,\n",
    "    'SD': 2,\n",
    "    'SMP': 3,\n",
    "    'SMA': 4,\n",
    "    'Diploma': 5,\n",
    "    'Pendidikan Tinggi': 6,\n",
    "}\n",
    "\n",
    "bulan_kontak_mapping = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "\n",
    "hasil_mapping = {\n",
    "    'failure': -1,\n",
    "    'nonexistent': 0,\n",
    "    'success': 1,\n",
    "}\n",
    "\n",
    "hari_kontak_mapping = {\n",
    "    'mon': 1, 'tue': 2, 'wed': 3, 'thu': 4, 'fri': 5\n",
    "}\n",
    "\n",
    "binary_mapping = {\n",
    "    'yes': 1,\n",
    "    'no': 0\n",
    "}\n",
    "\n",
    "train['pendidikan'] = train['pendidikan'].map(pendidikan_mapping)\n",
    "test['pendidikan'] = test['pendidikan'].map(pendidikan_mapping)\n",
    "\n",
    "train['bulan_kontak_terakhir'] = train['bulan_kontak_terakhir'].map(bulan_kontak_mapping)\n",
    "test['bulan_kontak_terakhir'] = test['bulan_kontak_terakhir'].map(bulan_kontak_mapping)\n",
    "\n",
    "train['hasil_kampanye_sebelumnya'] = train['hasil_kampanye_sebelumnya'].map(hasil_mapping)\n",
    "test['hasil_kampanye_sebelumnya'] = test['hasil_kampanye_sebelumnya'].map(hasil_mapping)\n",
    "\n",
    "train['hari_kontak_terakhir'] = train['hari_kontak_terakhir'].map(hari_kontak_mapping)\n",
    "test['hari_kontak_terakhir'] = test['hari_kontak_terakhir'].map(hari_kontak_mapping)\n",
    "\n",
    "train[['pinjaman_rumah', 'pinjaman_pribadi', 'gagal_bayar_sebelumnya']] = train[['pinjaman_rumah', 'pinjaman_pribadi', 'gagal_bayar_sebelumnya']].replace(binary_mapping)\n",
    "test[['pinjaman_rumah', 'pinjaman_pribadi', 'gagal_bayar_sebelumnya']] = test[['pinjaman_rumah', 'pinjaman_pribadi', 'gagal_bayar_sebelumnya']].replace(binary_mapping)\n",
    "\n",
    "train = one_hot_encode(train, ['pulau', 'jenis_kontak', 'status_perkawinan', 'pekerjaan'])\n",
    "test = one_hot_encode(test, ['pulau', 'jenis_kontak', 'status_perkawinan', 'pekerjaan'])\n",
    "\n",
    "# train = label_encode(train, 'pekerjaan')\n",
    "# test = label_encode(test, 'pekerjaan')\n",
    "\n",
    "train = train.astype('float32')\n",
    "test = test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e046a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b7c9d",
   "metadata": {},
   "source": [
    "## Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721c2f2",
   "metadata": {},
   "source": [
    "### Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dropped = train.dropna(axis=0, how='any')\n",
    "print(\"\\nTraining dataset after dropping rows with missing values:\")\n",
    "print(f'Ukuran dataset sebelum drop: {train.shape} dan ukuran dataset setelah drop: {train_dropped.shape}')\n",
    "\n",
    "test_dropped = test.dropna(axis=0, how='any')\n",
    "print(\"\\nValidation dataset after dropping rows with missing values:\")\n",
    "print(f'Ukuran dataset sebelum drop: {test.shape} dan ukuran dataset setelah drop: {test_dropped.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb162d05",
   "metadata": {},
   "source": [
    "### MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52930694",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice = IterativeImputer(random_state=42)\n",
    "\n",
    "train_imputed = mice.fit_transform(train)\n",
    "test_imputed = mice.fit_transform(test)\n",
    "\n",
    "train_imputed = pd.DataFrame(train_imputed, columns=train.columns)\n",
    "test_imputed = pd.DataFrame(test_imputed, columns=test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b15878",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd974fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_dropped.drop(columns=['berlangganan_deposito'])\n",
    "y = train_dropped['berlangganan_deposito']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37cd192",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a95ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984507c8",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc0442",
   "metadata": {},
   "source": [
    "## Over under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OVER UNDER\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# # Inisialisasi undersampler untuk kelas 0\n",
    "# undersampler = RandomUnderSampler(sampling_strategy={0: 5000}, random_state=42)\n",
    "# # Inisialisasi oversampler untuk kelas 1 dan 2\n",
    "# oversampler = RandomOverSampler(sampling_strategy={1: 2000, 0: 5000}, random_state=42)\n",
    "\n",
    "# X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# X_train_final, y_train_final = oversampler.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# from collections import Counter\n",
    "# print(\"Distribusi kelas setelah resampling:\", Counter(y_train_final['coppaRisk']))\n",
    "\n",
    "# X_train = X_train_final\n",
    "# y_train = y_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912a4e6",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "# X_val_resampled, y_val_resampled = smote.fit_resample(X_val_scaled, y_val)\n",
    "# print(\"Distribusi kelas setelah SMOTE:\")\n",
    "# print(Counter(y_train_resampled))\n",
    "# X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "# X_val_resampled = pd.DataFrame(X_val_resampled, columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee4fee",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a8106",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model1 = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"auc\", random_state=42)\n",
    "model1.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb34833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_importance(model1, max_num_features=20, importance_type='weight', title='Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec7407",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model1.predict(X_val_scaled)\n",
    "y_pred_proba = model1.predict_proba(X_val_scaled)[:,1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14992c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
