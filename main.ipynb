{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e55440d",
   "metadata": {},
   "source": [
    "# Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import sklearn\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LassoCV\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import optuna\n",
    "import shap\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"Seed set to {seed}\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/Data Quest Challenge DSI/training_dataset.csv')\n",
    "test = pd.read_csv('D:/Data Quest Challenge DSI/validation_set.csv')\n",
    "\n",
    "train.drop(['customer_number'], inplace=True, axis=1)\n",
    "test.drop(['customer_number'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.info())\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in both train and test datasets\n",
    "print(\"Missing values in training dataset:\")\n",
    "print(train.isna().sum())\n",
    "\n",
    "print(\"\\nMissing values in validation dataset:\")\n",
    "print(test.isna().sum())\n",
    "\n",
    "# Print total number of rows with missing values in each dataset\n",
    "print(\"\\nTotal rows with missing values:\")\n",
    "print(f\"Training dataset: {train.isna().any(axis=1).sum()} out of {train.shape[0]} rows\")\n",
    "print(f\"Validation dataset: {test.isna().any(axis=1).sum()} out of {test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb148c",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f889b7",
   "metadata": {},
   "source": [
    "gagal_bayar_sebelumnya cuman ada 2 baris yang classnya yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd0906",
   "metadata": {},
   "source": [
    "there's no class for yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abd1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = test.select_dtypes(include=['object']).columns.tolist()\n",
    "num_col = test.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", cat_col)\n",
    "print(f\"Jumlah kolom kategorikal: { len(cat_col) }\\n\")\n",
    "\n",
    "print(\"Numerical columns:\", num_col)\n",
    "print(\"Jumlah kolom numerikal:\", len(num_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique Value pendidikan:\", train['pendidikan'].unique())\n",
    "print(\"Unique Value pekerjaan:\", train['pekerjaan'].unique())\n",
    "print(\"Unique Value status_perkawinan:\", train['status_perkawinan'].unique())\n",
    "print(\"Unique Value bulan_kontak_terakhir:\", train['bulan_kontak_terakhir'].unique())\n",
    "print(\"Unique Value hari_kontak_terakhir:\", train['hari_kontak_terakhir'].unique())\n",
    "print(\"Unique Value pulau:\", train['pulau'].unique())\n",
    "\n",
    "# Lookin the distribution of each categorical variable\n",
    "def plot_categorical_distribution(df, column):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.countplot(data=df, x=column, order=df[column].value_counts().index)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_distribution(train, 'pendidikan')\n",
    "plot_categorical_distribution(train, 'pekerjaan')\n",
    "plot_categorical_distribution(train, 'status_perkawinan')\n",
    "plot_categorical_distribution(train, 'bulan_kontak_terakhir')\n",
    "plot_categorical_distribution(train, 'hari_kontak_terakhir')\n",
    "plot_categorical_distribution(train, 'pulau')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a0f76",
   "metadata": {},
   "source": [
    "## Transforming values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54409901",
   "metadata": {},
   "source": [
    "Merubah semua kategori unknown menjadi NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace unknown values with NaN\n",
    "train['pendidikan'] = train['pendidikan'].replace(['unknown'], np.nan)\n",
    "train['pekerjaan'] = train['pekerjaan'].replace(['unknown'], np.nan)\n",
    "train['status_perkawinan'] = train['status_perkawinan'].replace(['unknown'], np.nan)\n",
    "train['gagal_bayar_sebelumnya'] = train['gagal_bayar_sebelumnya'].replace(['unknown'], np.nan)\n",
    "train['pinjaman_rumah'] = train['pinjaman_rumah'].replace(['unknown'], np.nan)\n",
    "train['pinjaman_pribadi'] = train['pinjaman_pribadi'].replace(['unknown'], np.nan)\n",
    "train['pinjaman_pribadi'] = train['pinjaman_pribadi'].replace(['unknown'], np.nan)\n",
    "\n",
    "test['pendidikan'] = test['pendidikan'].replace(['unknown'], np.nan)\n",
    "test['pekerjaan'] = test['pekerjaan'].replace(['unknown'], np.nan)\n",
    "test['status_perkawinan'] = test['status_perkawinan'].replace(['unknown'], np.nan)\n",
    "test['gagal_bayar_sebelumnya'] = test['gagal_bayar_sebelumnya'].replace(['unknown'], np.nan)\n",
    "test['pinjaman_rumah'] = test['pinjaman_rumah'].replace(['unknown'], np.nan)\n",
    "test['pinjaman_pribadi'] = test['pinjaman_pribadi'].replace(['unknown'], np.nan)\n",
    "\n",
    "# Replace 999 with 0\n",
    "train['hari_sejak_kontak_sebelumnya'] = train['hari_sejak_kontak_sebelumnya'].replace([999], 0)\n",
    "test['hari_sejak_kontak_sebelumnya'] = test['hari_sejak_kontak_sebelumnya'].replace([999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd983dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in training dataset:\")\n",
    "print(train.isna().sum())\n",
    "print(\"Duplicate rows in training dataset:\")\n",
    "print(train.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values in validation dataset:\")\n",
    "print(test.isna().sum())\n",
    "print(\"Duplicate rows in validation dataset:\")\n",
    "print(test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87715aec",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0013ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop_duplicates(inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "print(\"Duplicate rows in training dataset after removal:\")\n",
    "print(train.duplicated().sum())\n",
    "print(\"Duplicate rows in validation dataset after removal:\")\n",
    "print(test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba625b4d",
   "metadata": {},
   "source": [
    "Outliers checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0894c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for numerical columns \n",
    "num_cols = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "print(\"Numerical columns in training dataset:\", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train['jumlah_pekerja'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold atas\n",
    "def replace_values_above_threshold(data_train, column, threshold):\n",
    "    sns.boxplot(data_train[column])\n",
    "    plt.title(f'Original Box Plot of {column}')\n",
    "    plt.show()\n",
    "\n",
    "    above_threshold = data_train[column] > threshold\n",
    "    data_train.loc[above_threshold, column] = threshold\n",
    "\n",
    "    sns.boxplot(data_train[column])\n",
    "    plt.title(f'Box Plot with Values Replaced above {threshold}')\n",
    "    plt.show()\n",
    "\n",
    "    return train\n",
    "\n",
    "# train = replace_values_above_threshold(train, 'usia', 68)\n",
    "# train = replace_values_above_threshold(train, 'jumlah_kontak_kampanye_ini', 6)\n",
    "# train = replace_values_above_threshold(train, 'indeks_kepercayaan_konsumen', -30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154c52e",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f36401",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "def one_hot_encode(df, columns):\n",
    "    return pd.get_dummies(df, columns=columns, drop_first=True)\n",
    "\n",
    "# Mapping Ordinal Columns\n",
    "pendidikan_mapping = {\n",
    "    'TIDAK SEKOLAH': 0,\n",
    "    'Tidak Tamat SD': 1,\n",
    "    'SD': 2,\n",
    "    'SMP': 3,\n",
    "    'SMA': 4,\n",
    "    'Diploma': 5,\n",
    "    'Pendidikan Tinggi': 6,\n",
    "}\n",
    "\n",
    "bulan_kontak_mapping = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "\n",
    "hasil_mapping = {\n",
    "    'failure': -1,\n",
    "    'nonexistent': 0,\n",
    "    'success': 1,\n",
    "}\n",
    "\n",
    "hari_kontak_mapping = {\n",
    "    'mon': 1, 'tue': 2, 'wed': 3, 'thu': 4, 'fri': 5\n",
    "}\n",
    "\n",
    "binary_mapping = {\n",
    "    'yes': 1,\n",
    "    'no': 0\n",
    "}\n",
    "\n",
    "# train['pendidikan'] = train['pendidikan'].map(pendidikan_mapping)\n",
    "# test['pendidikan'] = test['pendidikan'].map(pendidikan_mapping)\n",
    "\n",
    "# train['bulan_kontak_terakhir'] = train['bulan_kontak_terakhir'].map(bulan_kontak_mapping)\n",
    "# test['bulan_kontak_terakhir'] = test['bulan_kontak_terakhir'].map(bulan_kontak_mapping)\n",
    "\n",
    "# train['hasil_kampanye_sebelumnya'] = train['hasil_kampanye_sebelumnya'].map(hasil_mapping)\n",
    "# test['hasil_kampanye_sebelumnya'] = test['hasil_kampanye_sebelumnya'].map(hasil_mapping)\n",
    "\n",
    "# train['hari_kontak_terakhir'] = train['hari_kontak_terakhir'].map(hari_kontak_mapping)\n",
    "# test['hari_kontak_terakhir'] = test['hari_kontak_terakhir'].map(hari_kontak_mapping)\n",
    "\n",
    "# train[['pinjaman_rumah', 'pinjaman_pribadi', 'gagal_bayar_sebelumnya']] = train[['pinjaman_rumah', 'pinjaman_pribadi', 'gagal_bayar_sebelumnya']].replace(binary_mapping)\n",
    "# test[['pinjaman_rumah', 'pinjaman_pribadi', 'gagal_bayar_sebelumnya']] = test[['pinjaman_rumah', 'pinjaman_pribadi', 'gagal_bayar_sebelumnya']].replace(binary_mapping)\n",
    "\n",
    "# train = one_hot_encode(train, ['pulau', 'jenis_kontak', 'status_perkawinan'])\n",
    "# test = one_hot_encode(test, ['pulau', 'jenis_kontak', 'status_perkawinan'])\n",
    "# train = one_hot_encode(train, ['pekerjaan'])\n",
    "# test = one_hot_encode(test, ['pekerjaan'])\n",
    "\n",
    "for col in cat_col:\n",
    "    if col in train.columns:\n",
    "        train[col] = le.fit_transform(train[col])\n",
    "    if col in test.columns:\n",
    "        test[col] = le.transform(test[col])\n",
    "\n",
    "# train = train.astype('float32')\n",
    "# test = test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e046a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b7c9d",
   "metadata": {},
   "source": [
    "## Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721c2f2",
   "metadata": {},
   "source": [
    "### Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dropped = train.dropna(axis=0, how='any')\n",
    "print(\"\\nTraining dataset after dropping rows with missing values:\")\n",
    "print(f'Ukuran dataset sebelum drop: {train.shape} dan ukuran dataset setelah drop: {train_dropped.shape}')\n",
    "\n",
    "test_dropped = test.dropna(axis=0, how='any')\n",
    "print(\"\\nValidation dataset after dropping rows with missing values:\")\n",
    "print(f'Ukuran dataset sebelum drop: {test.shape} dan ukuran dataset setelah drop: {test_dropped.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb162d05",
   "metadata": {},
   "source": [
    "### MICE/KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = train_dropped.drop(['berlangganan_deposito'], axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52930694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize imputers\n",
    "mice = IterativeImputer(random_state=42)\n",
    "knn = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "# Fit MICE imputer on training data and transform both datasets\n",
    "train_imputed = mice.fit_transform(train[feature_cols])\n",
    "test_imputed = mice.transform(test)\n",
    "\n",
    "# Convert back to DataFrame with original column names\n",
    "train_imputed = pd.DataFrame(train_imputed, columns=train.drop(columns=['berlangganan_deposito']).columns)\n",
    "train_imputed['berlangganan_deposito'] = train['berlangganan_deposito'].values\n",
    "test_imputed = pd.DataFrame(test_imputed, columns=test.columns)\n",
    "\n",
    "# Print shape of imputed datasets\n",
    "print(f\"Training dataset after imputation: {train_imputed.shape}\")\n",
    "print(f\"Test dataset after imputation: {test_imputed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b15878",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd974fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['berlangganan_deposito'])\n",
    "y = train['berlangganan_deposito']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "def create_polynomial_features(df, poly):\n",
    "    poly_features = poly.fit_transform(df.select_dtypes(include=[np.number]))\n",
    "    poly_feature_names = poly.get_feature_names_out(df.select_dtypes(include=[np.number]).columns)\n",
    "    poly_df = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
    "    return poly_df\n",
    " \n",
    "X_poly = poly.fit_transform(X.select_dtypes(include=[np.number]))\n",
    "X_poly = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.select_dtypes(include=[np.number]).columns))\n",
    "X_poly = pd.concat([X.drop(columns=X.select_dtypes(include=[np.number]).columns), X_poly], axis=1)\n",
    "\n",
    "test_poly = poly.transform(test_imputed.select_dtypes(include=[np.number]))\n",
    "test_poly = pd.DataFrame(test_poly, columns=poly.get_feature_names_out(test_imputed.select_dtypes(include=[np.number]).columns))\n",
    "test_poly = pd.concat([test_imputed.drop(columns=test_imputed.select_dtypes(include=[np.number]).columns), test_poly], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ec18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_poly, y, test_size=0.25, stratify=y, random_state=42, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37cd192",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a95ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "robust = RobustScaler()\n",
    "X_train_scaled = robust.fit_transform(X_train)\n",
    "X_val_scaled = robust.transform(X_val)\n",
    "test_scaled = robust.transform(test_poly)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "test_scaled = pd.DataFrame(test_scaled, columns=test_poly.columns)\n",
    "\n",
    "X_train_scaled[cat_col] = X_train_scaled[cat_col].astype('int32')\n",
    "X_val_scaled[cat_col] = X_val_scaled[cat_col].astype('int32')\n",
    "test_scaled[cat_col] = test_scaled[cat_col].astype('int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984507c8",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc0442",
   "metadata": {},
   "source": [
    "## Over under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OVER UNDER\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# # Inisialisasi undersampler untuk kelas 0\n",
    "# undersampler = RandomUnderSampler(sampling_strategy={0: 5000}, random_state=42)\n",
    "# # Inisialisasi oversampler untuk kelas 1 dan 2\n",
    "# oversampler = RandomOverSampler(sampling_strategy={1: 2000, 0: 5000}, random_state=42)\n",
    "\n",
    "# X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# X_train_final, y_train_final = oversampler.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# from collections import Counter\n",
    "# print(\"Distribusi kelas setelah resampling:\", Counter(y_train_final['coppaRisk']))\n",
    "\n",
    "# X_train = X_train_final\n",
    "# y_train = y_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912a4e6",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "# X_val_resampled, y_val_resampled = smote.fit_resample(X_val_scaled, y_val)\n",
    "# print(\"Distribusi kelas setelah SMOTE:\")\n",
    "# print(Counter(y_train_resampled))\n",
    "# X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "# X_val_resampled = pd.DataFrame(X_val_resampled, columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee4fee",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a8106",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf97d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use elasticnet with saga solver and specify l1_ratio\n",
    "# # For elasticnet, we need to specify both solver='saga' and l1_ratio parameter\n",
    "# model3 = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000, random_state=42)\n",
    "# model3.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ec5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CatBoostClassifier(eval_metric='AUC', cat_features=cat_col, random_seed=42, verbose=0)\n",
    "model2.fit(X_train_scaled, y_train, eval_set=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# model1 = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"auc\", loss='logloss', random_state=42, use_label_encoder=False, verbosity=0)\n",
    "# model1.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb34833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import plot_importance\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plot_importance(model1, max_num_features=20, importance_type='weight', title='Feature Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec7407",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model2.predict(X_val_scaled)\n",
    "y_pred_proba = model2.predict_proba(X_val_scaled)[:,1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions\n",
    "# y_pred = model1.predict(X_val_scaled)   \n",
    "# y_pred_proba = model1.predict_proba(X_val_scaled)[:,1]\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "# conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# cm = confusion_matrix(y_val, y_pred)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['False', 'True'], yticklabels=['False', 'True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions\n",
    "# y_pred = model3.predict(X_val_scaled)\n",
    "# y_pred_proba = model3.predict_proba(X_val_scaled)[:,1]\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "# conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# cm = confusion_matrix(y_val, y_pred)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc01ae8",
   "metadata": {},
   "source": [
    "# Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a63104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature importance\n",
    "# explainer = shap.Explainer(model1)\n",
    "# shap_values = explainer.shap_values(X_val_scaled)\n",
    "# shap.summary_plot(shap_values, X_val_scaled, plot_type=\"bar\", max_display=20)\n",
    "\n",
    "# plot_importance(model1, max_num_features=20, importance_type='weight', title='Feature Importance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e3fc3",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fceeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        # 'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS', 'No']),\n",
    "        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    # First choose boosting type\n",
    "    param['boosting_type'] = trial.suggest_categorical('boosting_type', ['Ordered', 'Plain'])\n",
    "    \n",
    "    # For Ordered boosting, we must use SymmetricTree\n",
    "    if param['boosting_type'] == 'Ordered':\n",
    "        param['grow_policy'] = 'SymmetricTree'\n",
    "    else:\n",
    "        param['grow_policy'] = trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide'])\n",
    "    \n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n",
    "\n",
    "    # Create model with current hyperparameters\n",
    "    model = CatBoostClassifier(**param, eval_metric='AUC')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train, \n",
    "        eval_set=[(X_val_scaled, y_val)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Get validation AUC score\n",
    "    preds = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "# Create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, timeout=600)  # Set timeout to 10 minutes\n",
    "\n",
    "# Display best parameters and score\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (AUC): {study.best_value:.5f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "best_params['random_seed'] = 42\n",
    "best_params['verbose'] = 0\n",
    "best_params['eval_metric'] = 'AUC'\n",
    "\n",
    "best_model = CatBoostClassifier(**best_params)\n",
    "best_model.fit(\n",
    "    X_train_scaled, y_train, \n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Evaluate the optimized model\n",
    "y_pred = best_model.predict(X_val_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "print(f\"\\nOptimized Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "            xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n",
    "plt.title('Confusion Matrix - Optimized CatBoost Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = best_model.get_feature_importance()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "plt.title('Top 20 Feature Importance - Optimized CatBoost Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the best model for later prediction\n",
    "model2_optimized = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba028aa7",
   "metadata": {},
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfe = RFE(estimator=model2_optimized, step=1)\n",
    "# rfe.fit(X_train_scaled, y_train)\n",
    "# rfe_support = rfe.support_\n",
    "# rfe_ranking = rfe.ranking_\n",
    "# rfe_features = X_train_scaled.columns[rfe_support]\n",
    "# print(\"RFE Selected Features:\")\n",
    "# print(rfe_features.tolist())\n",
    "# # Create a new DataFrame with RFE selected features\n",
    "# X_train_rfe = X_train_scaled[rfe_features]\n",
    "# X_val_rfe = X_val_scaled[rfe_features]\n",
    "# test_rfe = test_scaled[rfe_features]\n",
    "\n",
    "# X_train_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_rfe = model2_optimized\n",
    "# model_rfe.fit(X_train_rfe, y_train, eval_set=(X_val_rfe, y_val), early_stopping_rounds=50, verbose=False)\n",
    "# # Evaluate the RFE model\n",
    "# y_pred_rfe = model_rfe.predict(X_val_rfe)\n",
    "# y_pred_proba_rfe = model_rfe.predict_proba(X_val_rfe)[:, 1]\n",
    "# accuracy_rfe = accuracy_score(y_val, y_pred_rfe)\n",
    "# roc_auc_rfe = roc_auc_score(y_val, y_pred_proba_rfe)\n",
    "\n",
    "# print(f\"RFE Model Performance:\")\n",
    "# print(f\"Accuracy: {accuracy_rfe:.4f}\")\n",
    "# print(f\"ROC AUC Score: {roc_auc_rfe:.4f}\")\n",
    "\n",
    "# # Plot confusion matrix for RFE model\n",
    "# cm_rfe = confusion_matrix(y_val, y_pred_rfe)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm_rfe, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "#             xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n",
    "# plt.title('Confusion Matrix - RFE CatBoost Model')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32666682",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e43c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model2_optimized.save_model('catboost_model_rfe3.cbm')\n",
    "\n",
    "# Save the parameters\n",
    "with open('catboost_params_rfe3.txt', 'w') as f:\n",
    "    for key, value in best_params.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id Column\n",
    "test_id = pd.read_csv('D:/Data Quest Challenge DSI/validation_set.csv')['customer_number']\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model2_optimized.predict_proba(test_scaled)[:, 1]\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'customer_number': test_id,\n",
    "    'berlangganan_deposito': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission.to_csv('submission4.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
